---
title: "Лабораторна робота №4"
author: "Конько Ярослав, КН-24-1"
output: html_notebook
frontsize: 15pt
editor_options: 
  chunk_output_type: inline
bibliography: references.bib
nocite: '@*'
---

# Тема

Перевірка статистичних гіпотез щодо закону розподілу. Перевірка на нормальність

# Мета роботи

Засвоїти ідею методики перевірки статистичних гіпотез щодо закону розподілу випадкової величини засобами мови програмування R і, зокрема, на нормальність; набути навичок роботи в середовищі RStudio із застосуванням концепції «грамотного програмування» із застосуванням пакету RMarkdown.

# Хід роботи

## 1. Постановка завдання з детальним описом заданого розподілу і його параметрів

Випадкова величина $X$ має **розподіл Коші**. Функція щільності розподілу Коші задана у вигляді:

$$f(x) = \frac{c}{1 + x^2}$$

де $c$ – деяка константа.

**Що треба зробити:**

1.  Знайти константу $c$

2.  Знайти функцію розподілу Коші $F(x)$

3.  Обчислити ймовірність події $-1 \leq X \leq 1$

## 2. Розв'язок теоретичної частини

### 1. Знаходження константи c
 Використовуємо умову нормування для функції щільності ймовірності:

$$\int_{-\infty}^{\infty} f(x) dx = 1$$

$$\int_{-\infty}^{\infty} \frac{c}{1 + x^2} dx = 1$$

Відомо, що:

$$\int_{-\infty}^{\infty} \frac{1}{1 + x^2} dx = \pi$$

Отже:

$$c \cdot \pi = 1 \Rightarrow c = \frac{1}{\pi}$$

**Відповідь:** $c = \frac{1}{\pi}$

Функція щільності розподілу Коші:

$$f(x) = \frac{1}{\pi(1 + x^2)}$$

### 2. Знаходження функції розподілу F(x)

Функція розподілу:

$$F(x) = \int_{-\infty}^{x} f(t) dt = \frac{1}{\pi} \int_{-\infty}^{x} \frac{1}{1 + t^2} dt$$

$$\int \frac{1}{1 + t^2} dt = \arctan(t) + C$$

Отже:

$$F(x) = \frac{1}{\pi} \left[ \arctan(t) \right]_{-\infty}^{x} = \frac{1}{\pi} \left( \arctan(x) - (-\frac{\pi}{2}) \right)$$

$$F(x) = \frac{1}{\pi} \arctan(x) + \frac{1}{2}$$

**Відповідь:** $F(x) = \frac{1}{\pi} \arctan(x) + \frac{1}{2}$

### 3. Обчислення ймовірності події $-1 \leq X \leq 1$

$$P(-1 \leq X \leq 1) = F(1) - F(-1)$$

$$F(1) = \frac{1}{\pi} \arctan(1) + \frac{1}{2} = \frac{1}{\pi} \cdot \frac{\pi}{4} + \frac{1}{2} = \frac{1}{4} + \frac{1}{2} = \frac{3}{4}$$

$$F(-1) = \frac{1}{\pi} \arctan(-1) + \frac{1}{2} = \frac{1}{\pi} \cdot (-\frac{\pi}{4}) + \frac{1}{2} = -\frac{1}{4} + \frac{1}{2} = \frac{1}{4}$$

$$P(-1 \leq X \leq 1) = \frac{3}{4} - \frac{1}{4} = \frac{1}{2}$$

**Відповідь:** $P(-1 \leq X \leq 1) = \frac{1}{2}$

## 3. Розв'язок практичної частини

### 1. Генерація вибірок з розподілу Коші
```{r}
set.seed(123)  # для відтворюваності результатів

n1 <- 50
n2 <- 1000

# Генеруємо вибірки з розподілу Коші (стандартний розподіл Коші: location=0, scale=1)
sample_50 <- rcauchy(n1, location = 0, scale = 1)
sample_1000 <- rcauchy(n2, location = 0, scale = 1)
```

### 2. Функції для розподілу Коші
```{r}
# Функція щільності розподілу Коші
dcauchy_custom <- function(x) {
  1 / (pi * (1 + x^2))
}

# Функція розподілу Коші
pcauchy_custom <- function(x) {
  atan(x) / pi + 0.5
}
```

### 3. Описова статистика
```{r}
desc_stats <- function(x) {
  c(
    n = length(x),
    mean = mean(x),
    median = median(x),
    sd = sd(x),
    min = min(x),
    max = max(x)
  )
}

stats_50 <- desc_stats(sample_50)
stats_1000 <- desc_stats(sample_1000)

knitr::kable(
  data.frame(
    Характеристика = names(stats_50),
    `n=50` = round(stats_50, 4),
    `n=1000` = round(stats_1000, 4)
  ),
  caption = "Основні статистичні характеристики вибірок"
)
```

### 4. Візуалізація розподілів
```{r}
# Функція для побудови графіків
plot_cauchy <- function(x, main = "") {
  par(mfrow = c(2, 2))
  
  # Гістограма з теоретичною щільністю
  hist(x, freq = FALSE, main = paste(main, "- Гістограма"), 
       col = "lightblue", breaks = 30, xlim = c(-10, 10))
  curve(dcauchy_custom(x), add = TRUE, col = "red", lwd = 2)
  
  # Q-Q plot порівняно з розподілом Коші
  qqplot(rcauchy(length(x)), x, 
         main = paste(main, "- Q-Q plot (Коші)"),
         xlab = "Теоретичні квантилі Коші",
         ylab = "Вибіркові квантилі")
  abline(0, 1, col = "red")
  
  # Емпірична функція розподілу
  plot(ecdf(x), main = paste(main, "- Емпірична F(x)"), 
       xlim = c(-5, 5))
  curve(pcauchy_custom(x), add = TRUE, col = "red")
  legend("topleft", legend = c("Емпірична", "Теоретична"), 
         col = c("black", "red"), lty = 1)
  
  # Boxplot (обрізаємо екстремальні значення для кращого відображення)
  boxplot(x, main = paste(main, "- Boxplot"), outline = FALSE)
}

plot_cauchy(sample_50, "n = 50")
```

### 5. Перевірка гіпотези про розподіл Коші
```{r}
# Тест Колмогорова-Смирнова для перевірки відповідності розподілу Коші
ks_test_50 <- ks.test(sample_50, "pcauchy", location = 0, scale = 1)
ks_test_1000 <- ks.test(sample_1000, "pcauchy", location = 0, scale = 1)

# Тест Андерсона-Дарлінга (потрібно встановити пакет goftest)
if (!require(goftest)) {
  install.packages("goftest")
  library(goftest)
}

ad_test_50 <- ad.test(sample_50, "pcauchy", location = 0, scale = 1)
ad_test_1000 <- ad.test(sample_1000, "pcauchy", location = 0, scale = 1)

# Формуємо таблицю результатів
test_results <- data.frame(
  Тест = c("Kolmogorov-Smirnov", "Anderson-Darling"),
  `n=50` = c(ks_test_50$p.value, ad_test_50$p.value),
  `n=1000` = c(ks_test_1000$p.value, ad_test_1000$p.value)
)

knitr::kable(
  test_results,
  caption = "p-value статистичних тестів на відповідність розподілу Коші",
  digits = 4
)
```

### 6. Емпірична перевірка ймовірності P(-1 ≤ X ≤ 1)
```{r}
# Теоретична ймовірність
theoretical_prob <- 0.5

# Емпіричні ймовірності
empirical_prob_50 <- mean(sample_50 >= -1 & sample_50 <= 1)
empirical_prob_1000 <- mean(sample_1000 >= -1 & sample_1000 <= 1)

prob_comparison <- data.frame(
  Вибірка = c("n = 50", "n = 1000", "Теоретична"),
  Ймовірність = c(empirical_prob_50, empirical_prob_1000, theoretical_prob)
)

knitr::kable(
  prob_comparison,
  caption = "Порівняння теоретичної та емпіричної ймовірності P(-1 ≤ X ≤ 1)",
  digits = 4
)
```

### 7. Висновки щодо гіпотези
```{r}
alpha <- 0.05

conclusion_50 <- ifelse(ks_test_50$p.value > alpha, "Не відхиляємо H0", "Відхиляємо H0")
conclusion_1000 <- ifelse(ks_test_1000$p.value > alpha, "Не відхиляємо H0", "Відхиляємо H0")

cat("Для вибірки n = 50:", conclusion_50, "(p-value =", round(ks_test_50$p.value, 4), ")\n")
cat("Для вибірки n = 1000:", conclusion_1000, "(p-value =", round(ks_test_1000$p.value, 4), ")\n")
cat("\nЕмпірична перевірка ймовірності P(-1 ≤ X ≤ 1):\n")
cat("Теоретична:", theoretical_prob, "\n")
cat("n = 50:", empirical_prob_50, "\n")
cat("n = 1000:", empirical_prob_1000)
```

### 8. Самостійне завдання
```{r}
# Власна реалізація функції щільності розподілу Коші
my_dcauchy <- function(x, location = 0, scale = 1) {
  1 / (pi * scale * (1 + ((x - location) / scale)^2))
}

# Власна реалізація функції розподілу Коші
my_pcauchy <- function(x, location = 0, scale = 1) {
  atan((x - location) / scale) / pi + 0.5
}

# Перевірка роботи функцій
x_test <- seq(-3, 3, length.out = 10)
comparison <- data.frame(
  x = x_test,
  dcauchy_R = dcauchy(x_test),
  dcauchy_my = my_dcauchy(x_test),
  pcauchy_R = pcauchy(x_test),
  pcauchy_my = my_pcauchy(x_test)
)

knitr::kable(
  comparison,
  caption = "Порівняння власних реалізацій з функціями R",
  digits = 6
)
```

## 4. Висновки
Теоретичні результати:

- Константа нормування: $c = \frac{1}{\pi}$

- Функція розподілу: $F(x) = \frac{1}{\pi} \arctan(x) + \frac{1}{2}$

- Ймовірність $P(-1 \leq X \leq 1) = \frac{1}{2}$

Емпіричні результати:

- Для обох вибірок p-value > 0.05, тому немає підстав відхилити H0 про розподіл Коші

- Емпірична ймовірність $P(-1 \leq X \leq 1)$ наближається до теоретичного значення 0.5 зі збільшенням обсягу вибірки

- Вибірка більшого обсягу (n=1000) краще відповідає теоретичному розподілу

Особливості розподілу Коші:

- Не має математичного сподівання та дисперсії

- Важкі "хвости" розподілу

- Медіана є стійкою характеристикою центру розподіл

## 5. Контрольні питання

### 1.  Які основні етапи перевірки статистичних гіпотез щодо закону розподілу і чому це важливо для статистичного аналізу?

- Формулювання H₀ (відповідність розподілу) та H₁

- Вибір рівня значущості (α=0.05)

- Обчислення статистики тесту (Shapiro-Wilk, KS тощо)

- Порівняння p-value з α

- Висновок: p-value < α - відхиляємо H₀

Важливість: коректність статистичних висновків залежить від відповідності даних припущенням про розподіл.

### 2.  Як використовувати R для визначення нормальності розподілу даних? Які функції або тести можна використовувати для цієї мети?
```{r}
shapiro.test(x)     # Основний тест
qqnorm(x); qqline(x)# Q-Q графік
ks.test(x, "pnorm") # Тест Колмогорова-Смирнова
```


### 3.  Як використовувати графічні методи, такі як QQ-графік, для оцінки нормальності розподілу даних у R?

Використовують **qqnorm(дані)** та **qqline(дані)** та точки мають лежати приблизно на прямій → розподіл близький до нормального.


### 4.  Які можливі наслідки або дії, якщо дані не відповідають припущенню про нормальний розподіл?

- Зниження потужності статистичних тестів

- Збільшення ймовірності помилок I та II роду

- Невірні довірчі інтервали та p-values

- Некоректні результати параметричних тестів (t-тест, ANOVA)

# Перелік посилань
